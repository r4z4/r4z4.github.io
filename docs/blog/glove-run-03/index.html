<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <link rel="stylesheet" href="https://r4z4.github.io/global.css">
  <title>Me</title>
</head>

<body>
  <nav><ul><li><a href="https://r4z4.github.io">Home</a></li></ul></nav>
  <section class="section">
    <div class="container">
      
<h1 class="title">
  GloVe Run 03
</h1>
<p class="subtitle"><strong>2023-03-05</strong></p>
<p>We are continuing to augment our data here. Now we will move on to the next simple augmentation techinque: Synonym Insertion. We simply find a synonym for a random word in the text, and add it in.</p>
<hr />
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#b48ead;">import </span><span>numpy </span><span style="color:#b48ead;">as </span><span>np
</span><span style="color:#b48ead;">import </span><span>regex </span><span style="color:#b48ead;">as </span><span>re
</span><span style="color:#b48ead;">import </span><span>pandas </span><span style="color:#b48ead;">as </span><span>pd
</span><span style="color:#b48ead;">from </span><span>tqdm.notebook </span><span style="color:#b48ead;">import </span><span>tqdm
</span><span style="color:#b48ead;">import </span><span>matplotlib.pyplot </span><span style="color:#b48ead;">as </span><span>plt
</span><span style="color:#b48ead;">import </span><span>seaborn </span><span style="color:#b48ead;">as </span><span>sns
</span><span style="color:#b48ead;">from </span><span>sklearn </span><span style="color:#b48ead;">import </span><span>preprocessing
</span><span style="color:#b48ead;">from </span><span>sklearn.model_selection </span><span style="color:#b48ead;">import </span><span>train_test_split
</span><span style="color:#b48ead;">import </span><span>utils
</span><span>
</span><span style="color:#b48ead;">import </span><span>keras
</span><span style="color:#b48ead;">import </span><span>tensorflow </span><span style="color:#b48ead;">as </span><span>tf
</span><span style="color:#b48ead;">from </span><span>keras.preprocessing.text </span><span style="color:#b48ead;">import </span><span>Tokenizer
</span><span style="color:#b48ead;">from </span><span>keras.utils </span><span style="color:#b48ead;">import </span><span>pad_sequences
</span></code></pre>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span>df = pd.</span><span style="color:#bf616a;">read_pickle</span><span>(&#39;</span><span style="color:#a3be8c;">./data/dataframes/outer_merged_normalized_deduped.pkl</span><span>&#39;)
</span><span>df.</span><span style="color:#bf616a;">head</span><span>()
</span></code></pre>
<hr />
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span>all_categories = [&#39;</span><span style="color:#a3be8c;">sport</span><span>&#39;, &#39;</span><span style="color:#a3be8c;">autos</span><span>&#39;, &#39;</span><span style="color:#a3be8c;">religion</span><span>&#39;, &#39;</span><span style="color:#a3be8c;">comp_elec</span><span>&#39;, &#39;</span><span style="color:#a3be8c;">sci_med</span><span>&#39;, &#39;</span><span style="color:#a3be8c;">seller</span><span>&#39;, &#39;</span><span style="color:#a3be8c;">politics</span><span>&#39;]
</span><span style="color:#65737e;"># We&#39;ll use all
</span><span>target_categories = [&#39;</span><span style="color:#a3be8c;">sport</span><span>&#39;, &#39;</span><span style="color:#a3be8c;">autos</span><span>&#39;, &#39;</span><span style="color:#a3be8c;">religion</span><span>&#39;, &#39;</span><span style="color:#a3be8c;">comp_elec</span><span>&#39;, &#39;</span><span style="color:#a3be8c;">sci_med</span><span>&#39;, &#39;</span><span style="color:#a3be8c;">seller</span><span>&#39;, &#39;</span><span style="color:#a3be8c;">politics</span><span>&#39;]
</span></code></pre>
<h3 id="augment-our-data">Augment our data</h3>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#65737e;"># But still need to fit the tokenizer on our original text to keep same vocak
</span><span>max_tokens = </span><span style="color:#d08770;">50 
</span><span>X = np.</span><span style="color:#bf616a;">array</span><span>([subject </span><span style="color:#b48ead;">for </span><span>subject </span><span style="color:#b48ead;">in </span><span>df[&#39;</span><span style="color:#a3be8c;">subject</span><span>&#39;]])
</span><span>tokenizer = </span><span style="color:#bf616a;">Tokenizer</span><span>()
</span><span>tokenizer.</span><span style="color:#bf616a;">fit_on_texts</span><span>(X)
</span></code></pre>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span>df[&#39;</span><span style="color:#a3be8c;">subject</span><span>&#39;] = df[&#39;</span><span style="color:#a3be8c;">subject</span><span>&#39;].</span><span style="color:#bf616a;">apply</span><span>(</span><span style="color:#b48ead;">lambda </span><span style="color:#bf616a;">x</span><span>: utils.</span><span style="color:#bf616a;">insert_rejoin</span><span>(x))
</span></code></pre>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#65737e;"># container for sentences
</span><span>X = np.</span><span style="color:#bf616a;">array</span><span>([subject </span><span style="color:#b48ead;">for </span><span>subject </span><span style="color:#b48ead;">in </span><span>df[&#39;</span><span style="color:#a3be8c;">subject</span><span>&#39;]])
</span><span style="color:#65737e;"># container for sentences
</span><span>y = np.</span><span style="color:#bf616a;">array</span><span>([subject </span><span style="color:#b48ead;">for </span><span>subject </span><span style="color:#b48ead;">in </span><span>df[&#39;</span><span style="color:#a3be8c;">newsgroup</span><span>&#39;]])
</span></code></pre>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span>encoder = preprocessing.</span><span style="color:#bf616a;">LabelEncoder</span><span>()
</span><span>y = encoder.</span><span style="color:#bf616a;">fit_transform</span><span>(df[&#39;</span><span style="color:#a3be8c;">newsgroup</span><span>&#39;])
</span></code></pre>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span>X_train, X_test, y_train, y_test = </span><span style="color:#bf616a;">train_test_split</span><span>(X, y,
</span><span>                                                    </span><span style="color:#bf616a;">stratify</span><span>=y, 
</span><span>                                                    </span><span style="color:#bf616a;">test_size</span><span>=</span><span style="color:#d08770;">0.25</span><span>)
</span><span>
</span><span>classes = np.</span><span style="color:#bf616a;">unique</span><span>(y_train)
</span><span>mapping = </span><span style="color:#bf616a;">dict</span><span>(</span><span style="color:#96b5b4;">zip</span><span>(classes, target_categories))
</span><span>
</span><span style="color:#96b5b4;">len</span><span>(X_train), </span><span style="color:#96b5b4;">len</span><span>(X_test), classes, mapping
</span></code></pre>
<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span>(6359,
</span><span> 2120,
</span><span> array([0, 1, 2, 3, 4, 5, 6]),
</span><span> {0: &#39;sport&#39;,
</span><span>  1: &#39;autos&#39;,
</span><span>  2: &#39;religion&#39;,
</span><span>  3: &#39;comp_elec&#39;,
</span><span>  4: &#39;sci_med&#39;,
</span><span>  5: &#39;seller&#39;,
</span><span>  6: &#39;politics&#39;})
</span></code></pre>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#65737e;">## Vectorizing data to keep 50 words per sample.
</span><span>X_train_vect = </span><span style="color:#bf616a;">pad_sequences</span><span>(tokenizer.</span><span style="color:#bf616a;">texts_to_sequences</span><span>(X_train), </span><span style="color:#bf616a;">maxlen</span><span>=max_tokens, </span><span style="color:#bf616a;">padding</span><span>=&quot;</span><span style="color:#a3be8c;">post</span><span>&quot;, </span><span style="color:#bf616a;">truncating</span><span>=&quot;</span><span style="color:#a3be8c;">post</span><span>&quot;, </span><span style="color:#bf616a;">value</span><span>=</span><span style="color:#d08770;">0.</span><span>)
</span><span>X_test_vect  = </span><span style="color:#bf616a;">pad_sequences</span><span>(tokenizer.</span><span style="color:#bf616a;">texts_to_sequences</span><span>(X_test), </span><span style="color:#bf616a;">maxlen</span><span>=max_tokens, </span><span style="color:#bf616a;">padding</span><span>=&quot;</span><span style="color:#a3be8c;">post</span><span>&quot;, </span><span style="color:#bf616a;">truncating</span><span>=&quot;</span><span style="color:#a3be8c;">post</span><span>&quot;, </span><span style="color:#bf616a;">value</span><span>=</span><span style="color:#d08770;">0.</span><span>)
</span><span>
</span><span style="color:#96b5b4;">print</span><span>(X_train_vect[:</span><span style="color:#d08770;">3</span><span>])
</span><span>
</span><span>X_train_vect.shape, X_test_vect.shape
</span></code></pre>
<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span>[[ 139  403  143 1947    2 7656    0    0    0    0    0    0    0    0
</span><span>     0    0    0    0    0    0    0    0    0    0    0    0    0    0
</span><span>     0    0    0    0    0    0    0    0    0    0    0    0    0    0
</span><span>     0    0    0    0    0    0    0    0]
</span><span> [8630   10 4127 4128    0    0    0    0    0    0    0    0    0    0
</span><span>     0    0    0    0    0    0    0    0    0    0    0    0    0    0
</span><span>     0    0    0    0    0    0    0    0    0    0    0    0    0    0
</span><span>     0    0    0    0    0    0    0    0]
</span><span> [   8  459 3023 4801  115   14    0    0    0    0    0    0    0    0
</span><span>     0    0    0    0    0    0    0    0    0    0    0    0    0    0
</span><span>     0    0    0    0    0    0    0    0    0    0    0    0    0    0
</span><span>     0    0    0    0    0    0    0    0]]
</span><span>
</span><span>
</span><span>
</span><span>
</span><span>
</span><span>((6359, 50), (2120, 50))
</span></code></pre>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#65737e;"># Should match previous runs
</span><span style="color:#96b5b4;">print</span><span>(&quot;</span><span style="color:#a3be8c;">Vocab Size : </span><span style="color:#d08770;">{}</span><span>&quot;.</span><span style="color:#bf616a;">format</span><span>(</span><span style="color:#96b5b4;">len</span><span>(tokenizer.word_index)))
</span></code></pre>
<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span>Vocab Size : 9734
</span></code></pre>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span>path = &#39;</span><span style="color:#a3be8c;">./glove.6B.50d.txt</span><span>&#39;
</span><span>glove_embeddings = {}
</span><span style="color:#b48ead;">with </span><span style="color:#96b5b4;">open</span><span>(path) </span><span style="color:#b48ead;">as </span><span>f:
</span><span>    </span><span style="color:#b48ead;">for </span><span>line </span><span style="color:#b48ead;">in </span><span>f:
</span><span>        </span><span style="color:#b48ead;">try</span><span>:
</span><span>            line = line.</span><span style="color:#bf616a;">split</span><span>()
</span><span>            glove_embeddings[line[</span><span style="color:#d08770;">0</span><span>]] = np.</span><span style="color:#bf616a;">array</span><span>(line[</span><span style="color:#d08770;">1</span><span>:], </span><span style="color:#bf616a;">dtype</span><span>=np.float32)
</span><span>        </span><span style="color:#b48ead;">except</span><span>:
</span><span>            </span><span style="color:#b48ead;">continue
</span></code></pre>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span>embed_len = </span><span style="color:#d08770;">50
</span><span>
</span><span>word_embeddings = np.</span><span style="color:#bf616a;">zeros</span><span>((</span><span style="color:#96b5b4;">len</span><span>(tokenizer.index_word)+</span><span style="color:#d08770;">1</span><span>, embed_len))
</span><span>
</span><span style="color:#b48ead;">for </span><span>idx, word </span><span style="color:#b48ead;">in </span><span>tokenizer.index_word.</span><span style="color:#bf616a;">items</span><span>():
</span><span>    word_embeddings[idx] = glove_embeddings.</span><span style="color:#bf616a;">get</span><span>(word, np.</span><span style="color:#bf616a;">zeros</span><span>(embed_len))
</span></code></pre>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span>word_embeddings[</span><span style="color:#d08770;">1</span><span>][:</span><span style="color:#d08770;">10</span><span>]
</span></code></pre>
<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span>array([ 0.15272   ,  0.36181   , -0.22168   ,  0.066051  ,  0.13029   ,
</span><span>        0.37075001, -0.75874001, -0.44722   ,  0.22563   ,  0.10208   ])
</span></code></pre>
<h3 id="approach-1-glove-embeddings-flattened-max-tokens-50-embedding-length-300">Approach 1: GloVe Embeddings Flattened (Max Tokens=50, Embedding Length=300)</h3>
<h4 id="load-previously-trained-model">Load previously trained model</h4>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#65737e;"># Load model
</span><span>model_file = &#39;</span><span style="color:#a3be8c;">models/sparse_cat_entire</span><span>&#39;
</span><span>model = keras.models.</span><span style="color:#bf616a;">load_model</span><span>(model_file)
</span><span>model.</span><span style="color:#bf616a;">summary</span><span>()
</span></code></pre>
<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span>Model: &quot;SparseCategoricalEntireData&quot;
</span><span>_________________________________________________________________
</span><span> Layer (type)                Output Shape              Param #   
</span><span>=================================================================
</span><span> embedding (Embedding)       (None, 50, 50)            486750    
</span><span>                                                                 
</span><span> flatten (Flatten)           (None, 2500)              0         
</span><span>                                                                 
</span><span> dense (Dense)               (None, 128)               320128    
</span><span>                                                                 
</span><span> dense_1 (Dense)             (None, 64)                8256      
</span><span>                                                                 
</span><span> dense_2 (Dense)             (None, 7)                 455       
</span><span>                                                                 
</span><span>=================================================================
</span><span>Total params: 815,589
</span><span>Trainable params: 328,839
</span><span>Non-trainable params: 486,750
</span><span>_________________________________________________________________
</span></code></pre>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span>model.weights[</span><span style="color:#d08770;">0</span><span>][</span><span style="color:#d08770;">1</span><span>][:</span><span style="color:#d08770;">10</span><span>], word_embeddings[</span><span style="color:#d08770;">1</span><span>][:</span><span style="color:#d08770;">10</span><span>]
</span></code></pre>
<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span>(&lt;tf.Tensor: shape=(10,), dtype=float32, numpy=
</span><span> array([ 0.15272 ,  0.36181 , -0.22168 ,  0.066051,  0.13029 ,  0.37075 ,
</span><span>        -0.75874 , -0.44722 ,  0.22563 ,  0.10208 ], dtype=float32)&gt;,
</span><span> array([ 0.15272   ,  0.36181   , -0.22168   ,  0.066051  ,  0.13029   ,
</span><span>         0.37075001, -0.75874001, -0.44722   ,  0.22563   ,  0.10208   ]))
</span></code></pre>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span>model.</span><span style="color:#bf616a;">fit</span><span>(X_train_vect, y_train, </span><span style="color:#bf616a;">batch_size</span><span>=</span><span style="color:#d08770;">32</span><span>, </span><span style="color:#bf616a;">epochs</span><span>=</span><span style="color:#d08770;">8</span><span>, </span><span style="color:#bf616a;">validation_data</span><span>=(X_test_vect, y_test))
</span></code></pre>
<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span>Epoch 1/8
</span><span>199/199 [==============================] - 2s 7ms/step - loss: 0.8672 - accuracy: 0.7254 - val_loss: 0.7931 - val_accuracy: 0.7401
</span><span>Epoch 2/8
</span><span>199/199 [==============================] - 1s 7ms/step - loss: 0.5856 - accuracy: 0.8140 - val_loss: 0.8054 - val_accuracy: 0.7373
</span><span>Epoch 3/8
</span><span>199/199 [==============================] - 1s 6ms/step - loss: 0.4570 - accuracy: 0.8589 - val_loss: 0.8408 - val_accuracy: 0.7392
</span><span>Epoch 4/8
</span><span>199/199 [==============================] - 1s 7ms/step - loss: 0.3649 - accuracy: 0.8909 - val_loss: 0.9048 - val_accuracy: 0.7203
</span><span>Epoch 5/8
</span><span>199/199 [==============================] - 1s 6ms/step - loss: 0.2945 - accuracy: 0.9089 - val_loss: 0.9471 - val_accuracy: 0.7297
</span><span>Epoch 6/8
</span><span>199/199 [==============================] - 1s 6ms/step - loss: 0.2276 - accuracy: 0.9344 - val_loss: 1.0200 - val_accuracy: 0.7259
</span><span>Epoch 7/8
</span><span>199/199 [==============================] - 1s 6ms/step - loss: 0.1874 - accuracy: 0.9475 - val_loss: 1.0948 - val_accuracy: 0.7113
</span><span>Epoch 8/8
</span><span>199/199 [==============================] - 1s 6ms/step - loss: 0.1449 - accuracy: 0.9616 - val_loss: 1.2295 - val_accuracy: 0.7066
</span><span>
</span><span>
</span><span>&lt;keras.callbacks.History at 0x7f5a9e0cad00&gt;
</span></code></pre>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#b48ead;">from </span><span>sklearn.metrics </span><span style="color:#b48ead;">import </span><span>accuracy_score, classification_report, confusion_matrix
</span><span>
</span><span>y_preds = model.</span><span style="color:#bf616a;">predict</span><span>(X_test_vect).</span><span style="color:#bf616a;">argmax</span><span>(</span><span style="color:#bf616a;">axis</span><span>=-</span><span style="color:#d08770;">1</span><span>)
</span><span>
</span><span style="color:#96b5b4;">print</span><span>(&quot;</span><span style="color:#a3be8c;">Test Accuracy : </span><span style="color:#d08770;">{}</span><span>&quot;.</span><span style="color:#bf616a;">format</span><span>(</span><span style="color:#bf616a;">accuracy_score</span><span>(y_test, y_preds)))
</span><span style="color:#96b5b4;">print</span><span>(&quot;</span><span style="color:#96b5b4;">\n</span><span style="color:#a3be8c;">Classification Report : </span><span>&quot;)
</span><span style="color:#96b5b4;">print</span><span>(</span><span style="color:#bf616a;">classification_report</span><span>(y_test, y_preds, </span><span style="color:#bf616a;">target_names</span><span>=target_categories))
</span><span style="color:#96b5b4;">print</span><span>(&quot;</span><span style="color:#96b5b4;">\n</span><span style="color:#a3be8c;">Confusion Matrix : </span><span>&quot;)
</span><span style="color:#96b5b4;">print</span><span>(</span><span style="color:#bf616a;">confusion_matrix</span><span>(y_test, y_preds))
</span></code></pre>
<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span>67/67 [==============================] - 0s 3ms/step
</span><span>Test Accuracy : 0.7066037735849057
</span><span>
</span><span>Classification Report : 
</span><span>              precision    recall  f1-score   support
</span><span>
</span><span>       sport       0.62      0.42      0.50       189
</span><span>       autos       0.76      0.83      0.79       901
</span><span>    religion       0.70      0.61      0.65       220
</span><span>   comp_elec       0.60      0.63      0.61       179
</span><span>     sci_med       0.65      0.61      0.63       192
</span><span>      seller       0.64      0.68      0.66       221
</span><span>    politics       0.75      0.72      0.73       218
</span><span>
</span><span>    accuracy                           0.71      2120
</span><span>   macro avg       0.67      0.64      0.65      2120
</span><span>weighted avg       0.70      0.71      0.70      2120
</span><span>
</span><span>
</span><span>Confusion Matrix : 
</span><span>[[ 79  48   9  11  11  26   5]
</span><span> [ 15 749  18  24  29  47  19]
</span><span> [  6  32 134  23   8   3  14]
</span><span> [  2  34  15 112   9   1   6]
</span><span> [  6  47   5   5 117   6   6]
</span><span> [  7  55   2   1   2 151   3]
</span><span> [ 12  23   8  12   4   3 156]]
</span></code></pre>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#65737e;"># TensorFlow SavedModel format =&gt; .keras
</span><span>model_file = &#39;</span><span style="color:#a3be8c;">models/sparse_cat_entire</span><span>&#39;
</span><span>model.</span><span style="color:#bf616a;">save</span><span>(model_file)
</span></code></pre>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#b48ead;">from </span><span>sklearn.metrics </span><span style="color:#b48ead;">import </span><span>confusion_matrix
</span><span style="color:#b48ead;">import </span><span>scikitplot </span><span style="color:#b48ead;">as </span><span>skplt
</span><span style="color:#b48ead;">import </span><span>matplotlib.pyplot </span><span style="color:#b48ead;">as </span><span>plt
</span><span style="color:#b48ead;">import </span><span>numpy </span><span style="color:#b48ead;">as </span><span>np
</span><span>
</span><span>skplt.metrics.</span><span style="color:#bf616a;">plot_confusion_matrix</span><span>([target_categories[i] </span><span style="color:#b48ead;">for </span><span>i </span><span style="color:#b48ead;">in </span><span>y_test], [target_categories[i] </span><span style="color:#b48ead;">for </span><span>i </span><span style="color:#b48ead;">in </span><span>y_preds],
</span><span>                                    </span><span style="color:#bf616a;">normalize</span><span>=</span><span style="color:#d08770;">True</span><span>,
</span><span>                                    </span><span style="color:#bf616a;">title</span><span>=&quot;</span><span style="color:#a3be8c;">Confusion Matrix</span><span>&quot;,
</span><span>                                    </span><span style="color:#bf616a;">cmap</span><span>=&quot;</span><span style="color:#a3be8c;">Greens</span><span>&quot;,
</span><span>                                    </span><span style="color:#bf616a;">hide_zeros</span><span>=</span><span style="color:#d08770;">True</span><span>,
</span><span>                                    </span><span style="color:#bf616a;">figsize</span><span>=(</span><span style="color:#d08770;">5</span><span>,</span><span style="color:#d08770;">5</span><span>)
</span><span>                                    );
</span><span>plt.</span><span style="color:#bf616a;">xticks</span><span>(</span><span style="color:#bf616a;">rotation</span><span>=</span><span style="color:#d08770;">90</span><span>);
</span></code></pre>
<p><img src="/images/glove/run_03.png#img-thumbnail" alt="png" /></p>
<h3 id="custom-test">Custom Test</h3>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#65737e;"># define documents
</span><span>docs = [&#39;</span><span style="color:#a3be8c;">Democrats the Reuplicans are both the worst!</span><span>&#39;,
</span><span>&#39;</span><span style="color:#a3be8c;">Coyotes win 10-0</span><span>&#39;,
</span><span>&#39;</span><span style="color:#a3be8c;">Houston Astros defeat the Cubs</span><span>&#39;,
</span><span>&#39;</span><span style="color:#a3be8c;">Apple and Microsoft both make great computers</span><span>&#39;,
</span><span>&#39;</span><span style="color:#a3be8c;">New washer 4sale. $200</span><span>&#39;]
</span><span>
</span><span>doc_array = np.</span><span style="color:#bf616a;">array</span><span>(docs)
</span><span>
</span><span>doc_array_vect  = </span><span style="color:#bf616a;">pad_sequences</span><span>(tokenizer.</span><span style="color:#bf616a;">texts_to_sequences</span><span>(doc_array), </span><span style="color:#bf616a;">maxlen</span><span>=max_tokens, </span><span style="color:#bf616a;">padding</span><span>=&quot;</span><span style="color:#a3be8c;">post</span><span>&quot;, </span><span style="color:#bf616a;">truncating</span><span>=&quot;</span><span style="color:#a3be8c;">post</span><span>&quot;, </span><span style="color:#bf616a;">value</span><span>=</span><span style="color:#d08770;">0.</span><span>)
</span><span>
</span><span>cstm_test_preds = model.</span><span style="color:#bf616a;">predict</span><span>(doc_array_vect).</span><span style="color:#bf616a;">argmax</span><span>(</span><span style="color:#bf616a;">axis</span><span>=-</span><span style="color:#d08770;">1</span><span>)
</span></code></pre>
<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span>1/1 [==============================] - 0s 21ms/step
</span></code></pre>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#96b5b4;">print</span><span>(doc_array)
</span></code></pre>
<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span>[&#39;Democrats the Reuplicans are both the worst!&#39; &#39;Coyotes win 10-0&#39;
</span><span> &#39;Houston Astros defeat the Cubs&#39;
</span><span> &#39;Apple and Microsoft both make great computers&#39; &#39;New washer 4sale. $200&#39;]
</span></code></pre>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#96b5b4;">print</span><span>(doc_array_vect)
</span></code></pre>
<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span>[[   2   53 3716    2 1299    0    0    0    0    0    0    0    0    0
</span><span>     0    0    0    0    0    0    0    0    0    0    0    0    0    0
</span><span>     0    0    0    0    0    0    0    0    0    0    0    0    0    0
</span><span>     0    0    0    0    0    0    0    0]
</span><span> [  69  199   47    0    0    0    0    0    0    0    0    0    0    0
</span><span>     0    0    0    0    0    0    0    0    0    0    0    0    0    0
</span><span>     0    0    0    0    0    0    0    0    0    0    0    0    0    0
</span><span>     0    0    0    0    0    0    0    0]
</span><span> [2931 4257    2 1113    0    0    0    0    0    0    0    0    0    0
</span><span>     0    0    0    0    0    0    0    0    0    0    0    0    0    0
</span><span>     0    0    0    0    0    0    0    0    0    0    0    0    0    0
</span><span>     0    0    0    0    0    0    0    0]
</span><span> [ 124    3  461 3716  277  580 1149    0    0    0    0    0    0    0
</span><span>     0    0    0    0    0    0    0    0    0    0    0    0    0    0
</span><span>     0    0    0    0    0    0    0    0    0    0    0    0    0    0
</span><span>     0    0    0    0    0    0    0    0]
</span><span> [  27  842  542    0    0    0    0    0    0    0    0    0    0    0
</span><span>     0    0    0    0    0    0    0    0    0    0    0    0    0    0
</span><span>     0    0    0    0    0    0    0    0    0    0    0    0    0    0
</span><span>     0    0    0    0    0    0    0    0]]
</span></code></pre>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#96b5b4;">print</span><span>(cstm_test_preds)
</span></code></pre>
<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span>[2 6 6 1 3]
</span></code></pre>


    </div>
  </section>
</body>

</html>