<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <link rel="stylesheet" href="https://r4z4.github.io/global.css">
  <title>Me</title>
</head>

<body>
  <nav><ul><li><a href="https://r4z4.github.io">Home</a></li></ul></nav>
  <section class="section">
    <div class="container">
      
<h1 class="title">
  GloVe Run 02
</h1>
<p class="subtitle"><strong>2023-03-04</strong></p>
<p>Now we will start to augment our data in order to get a better understand of what kinds of parameters matter and how they all interact. The benefit of keeping things relatively simple will allow us to see a lot more of the details that may be hidden away in some of the more complex examples that you may come across.</p>
<hr />
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#b48ead;">import </span><span>numpy </span><span style="color:#b48ead;">as </span><span>np
</span><span style="color:#b48ead;">import </span><span>regex </span><span style="color:#b48ead;">as </span><span>re
</span><span style="color:#b48ead;">import </span><span>pandas </span><span style="color:#b48ead;">as </span><span>pd
</span><span style="color:#b48ead;">from </span><span>tqdm.notebook </span><span style="color:#b48ead;">import </span><span>tqdm
</span><span style="color:#b48ead;">import </span><span>matplotlib.pyplot </span><span style="color:#b48ead;">as </span><span>plt
</span><span style="color:#b48ead;">import </span><span>seaborn </span><span style="color:#b48ead;">as </span><span>sns
</span><span style="color:#b48ead;">from </span><span>sklearn </span><span style="color:#b48ead;">import </span><span>preprocessing
</span><span style="color:#b48ead;">import </span><span>utils
</span><span>
</span><span style="color:#b48ead;">import </span><span>keras
</span><span style="color:#b48ead;">import </span><span>tensorflow </span><span style="color:#b48ead;">as </span><span>tf
</span><span style="color:#b48ead;">from </span><span>keras.preprocessing.text </span><span style="color:#b48ead;">import </span><span>Tokenizer
</span><span style="color:#b48ead;">from </span><span>keras.utils </span><span style="color:#b48ead;">import </span><span>pad_sequences
</span></code></pre>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span>df = pd.</span><span style="color:#bf616a;">read_pickle</span><span>(&#39;</span><span style="color:#a3be8c;">./data/dataframes/outer_merged_normalized_deduped.pkl</span><span>&#39;)
</span><span>df.</span><span style="color:#bf616a;">head</span><span>()
</span></code></pre>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span>all_categories = [&#39;</span><span style="color:#a3be8c;">sport</span><span>&#39;, &#39;</span><span style="color:#a3be8c;">autos</span><span>&#39;, &#39;</span><span style="color:#a3be8c;">religion</span><span>&#39;, &#39;</span><span style="color:#a3be8c;">comp_elec</span><span>&#39;, &#39;</span><span style="color:#a3be8c;">sci_med</span><span>&#39;, &#39;</span><span style="color:#a3be8c;">seller</span><span>&#39;, &#39;</span><span style="color:#a3be8c;">politics</span><span>&#39;]
</span><span style="color:#65737e;"># We&#39;ll use all
</span><span>target_categories = [&#39;</span><span style="color:#a3be8c;">sport</span><span>&#39;, &#39;</span><span style="color:#a3be8c;">autos</span><span>&#39;, &#39;</span><span style="color:#a3be8c;">religion</span><span>&#39;, &#39;</span><span style="color:#a3be8c;">comp_elec</span><span>&#39;, &#39;</span><span style="color:#a3be8c;">sci_med</span><span>&#39;, &#39;</span><span style="color:#a3be8c;">seller</span><span>&#39;, &#39;</span><span style="color:#a3be8c;">politics</span><span>&#39;]
</span></code></pre>
<h3 id="augment-our-data">Augment our data</h3>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#65737e;"># But still need to fit the tokenizer on our original text to keep same vocak
</span><span>max_tokens = </span><span style="color:#d08770;">50 
</span><span>X = np.</span><span style="color:#bf616a;">array</span><span>([subject </span><span style="color:#b48ead;">for </span><span>subject </span><span style="color:#b48ead;">in </span><span>df[&#39;</span><span style="color:#a3be8c;">subject</span><span>&#39;]])
</span><span>tokenizer = </span><span style="color:#bf616a;">Tokenizer</span><span>()
</span><span>tokenizer.</span><span style="color:#bf616a;">fit_on_texts</span><span>(X)
</span></code></pre>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span>df[&#39;</span><span style="color:#a3be8c;">subject</span><span>&#39;] = df[&#39;</span><span style="color:#a3be8c;">subject</span><span>&#39;].</span><span style="color:#bf616a;">apply</span><span>(</span><span style="color:#b48ead;">lambda </span><span style="color:#bf616a;">x</span><span>: utils.</span><span style="color:#bf616a;">replace_rejoin</span><span>(x))
</span></code></pre>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#65737e;"># container for sentences
</span><span>X = np.</span><span style="color:#bf616a;">array</span><span>([subject </span><span style="color:#b48ead;">for </span><span>subject </span><span style="color:#b48ead;">in </span><span>df[&#39;</span><span style="color:#a3be8c;">subject</span><span>&#39;]])
</span><span style="color:#65737e;"># container for sentences
</span><span>y = np.</span><span style="color:#bf616a;">array</span><span>([subject </span><span style="color:#b48ead;">for </span><span>subject </span><span style="color:#b48ead;">in </span><span>df[&#39;</span><span style="color:#a3be8c;">newsgroup</span><span>&#39;]])
</span></code></pre>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span>encoder = preprocessing.</span><span style="color:#bf616a;">LabelEncoder</span><span>()
</span><span>y = encoder.</span><span style="color:#bf616a;">fit_transform</span><span>(df[&#39;</span><span style="color:#a3be8c;">newsgroup</span><span>&#39;])
</span></code></pre>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span>X_train, X_test, y_train, y_test = </span><span style="color:#bf616a;">train_test_split</span><span>(X, y,
</span><span>                                                    </span><span style="color:#bf616a;">stratify</span><span>=y, 
</span><span>                                                    </span><span style="color:#bf616a;">test_size</span><span>=</span><span style="color:#d08770;">0.25</span><span>)
</span><span>
</span><span>classes = np.</span><span style="color:#bf616a;">unique</span><span>(y_train)
</span><span>mapping = </span><span style="color:#bf616a;">dict</span><span>(</span><span style="color:#96b5b4;">zip</span><span>(classes, target_categories))
</span><span>
</span><span style="color:#96b5b4;">len</span><span>(X_train), </span><span style="color:#96b5b4;">len</span><span>(X_test), classes, mapping
</span></code></pre>
<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span>(28245,
</span><span> 9415,
</span><span> array([0, 1, 2, 3, 4, 5, 6]),
</span><span> {0: &#39;sport&#39;,
</span><span>  1: &#39;autos&#39;,
</span><span>  2: &#39;religion&#39;,
</span><span>  3: &#39;comp_elec&#39;,
</span><span>  4: &#39;sci_med&#39;,
</span><span>  5: &#39;seller&#39;,
</span><span>  6: &#39;politics&#39;})
</span></code></pre>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#65737e;">## Vectorizing data to keep 50 words per sample.
</span><span>X_train_vect = </span><span style="color:#bf616a;">pad_sequences</span><span>(tokenizer.</span><span style="color:#bf616a;">texts_to_sequences</span><span>(X_train), </span><span style="color:#bf616a;">maxlen</span><span>=max_tokens, </span><span style="color:#bf616a;">padding</span><span>=&quot;</span><span style="color:#a3be8c;">post</span><span>&quot;, </span><span style="color:#bf616a;">truncating</span><span>=&quot;</span><span style="color:#a3be8c;">post</span><span>&quot;, </span><span style="color:#bf616a;">value</span><span>=</span><span style="color:#d08770;">0.</span><span>)
</span><span>X_test_vect  = </span><span style="color:#bf616a;">pad_sequences</span><span>(tokenizer.</span><span style="color:#bf616a;">texts_to_sequences</span><span>(X_test), </span><span style="color:#bf616a;">maxlen</span><span>=max_tokens, </span><span style="color:#bf616a;">padding</span><span>=&quot;</span><span style="color:#a3be8c;">post</span><span>&quot;, </span><span style="color:#bf616a;">truncating</span><span>=&quot;</span><span style="color:#a3be8c;">post</span><span>&quot;, </span><span style="color:#bf616a;">value</span><span>=</span><span style="color:#d08770;">0.</span><span>)
</span><span>
</span><span style="color:#96b5b4;">print</span><span>(X_train_vect[:</span><span style="color:#d08770;">3</span><span>])
</span><span>
</span><span>X_train_vect.shape, X_test_vect.shape
</span></code></pre>
<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span>[[7186   20 4935    0    0    0    0    0    0    0    0    0    0    0
</span><span>     0    0    0    0    0    0    0    0    0    0    0    0    0    0
</span><span>     0    0    0    0    0    0    0    0    0    0    0    0    0    0
</span><span>     0    0    0    0    0    0    0    0]
</span><span> [ 933   38 2247 1972    8   84  281    0    0    0    0    0    0    0
</span><span>     0    0    0    0    0    0    0    0    0    0    0    0    0    0
</span><span>     0    0    0    0    0    0    0    0    0    0    0    0    0    0
</span><span>     0    0    0    0    0    0    0    0]
</span><span> [3365  641    0    0    0    0    0    0    0    0    0    0    0    0
</span><span>     0    0    0    0    0    0    0    0    0    0    0    0    0    0
</span><span>     0    0    0    0    0    0    0    0    0    0    0    0    0    0
</span><span>     0    0    0    0    0    0    0    0]]
</span><span>
</span><span>
</span><span>
</span><span>((28245, 50), (9415, 50))
</span></code></pre>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#96b5b4;">print</span><span>(&quot;</span><span style="color:#a3be8c;">Vocab Size : </span><span style="color:#d08770;">{}</span><span>&quot;.</span><span style="color:#bf616a;">format</span><span>(</span><span style="color:#96b5b4;">len</span><span>(tokenizer.word_index)))
</span></code></pre>
<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span>Vocab Size : 9734
</span></code></pre>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span>path = &#39;</span><span style="color:#a3be8c;">./glove.6B.50d.txt</span><span>&#39;
</span><span>glove_embeddings = {}
</span><span style="color:#b48ead;">with </span><span style="color:#96b5b4;">open</span><span>(path) </span><span style="color:#b48ead;">as </span><span>f:
</span><span>    </span><span style="color:#b48ead;">for </span><span>line </span><span style="color:#b48ead;">in </span><span>f:
</span><span>        </span><span style="color:#b48ead;">try</span><span>:
</span><span>            line = line.</span><span style="color:#bf616a;">split</span><span>()
</span><span>            glove_embeddings[line[</span><span style="color:#d08770;">0</span><span>]] = np.</span><span style="color:#bf616a;">array</span><span>(line[</span><span style="color:#d08770;">1</span><span>:], </span><span style="color:#bf616a;">dtype</span><span>=np.float32)
</span><span>        </span><span style="color:#b48ead;">except</span><span>:
</span><span>            </span><span style="color:#b48ead;">continue
</span></code></pre>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span>embed_len = </span><span style="color:#d08770;">50
</span><span>
</span><span>word_embeddings = np.</span><span style="color:#bf616a;">zeros</span><span>((</span><span style="color:#96b5b4;">len</span><span>(tokenizer.index_word)+</span><span style="color:#d08770;">1</span><span>, embed_len))
</span><span>
</span><span style="color:#b48ead;">for </span><span>idx, word </span><span style="color:#b48ead;">in </span><span>tokenizer.index_word.</span><span style="color:#bf616a;">items</span><span>():
</span><span>    word_embeddings[idx] = glove_embeddings.</span><span style="color:#bf616a;">get</span><span>(word, np.</span><span style="color:#bf616a;">zeros</span><span>(embed_len))
</span></code></pre>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span>word_embeddings[</span><span style="color:#d08770;">1</span><span>][:</span><span style="color:#d08770;">10</span><span>]
</span></code></pre>
<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span>array([ 0.15272   ,  0.36181   , -0.22168   ,  0.066051  ,  0.13029   ,
</span><span>        0.37075001, -0.75874001, -0.44722   ,  0.22563   ,  0.10208   ])
</span></code></pre>
<h3 id="approach-1-glove-embeddings-flattened-max-tokens-50-embedding-length-300">Approach 1: GloVe Embeddings Flattened (Max Tokens=50, Embedding Length=300)</h3>
<h3 id="load-previously-trained-model">Load previously trained model</h3>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#65737e;"># Load model
</span><span>model_file = &#39;</span><span style="color:#a3be8c;">models/sparse_cat_entire</span><span>&#39;
</span><span>model = keras.models.</span><span style="color:#bf616a;">load_model</span><span>(model_file)
</span><span>model._name = &quot;</span><span style="color:#a3be8c;">SparseCategoricalEntireData</span><span>&quot;
</span><span>model.</span><span style="color:#bf616a;">summary</span><span>()
</span></code></pre>
<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span>Model: &quot;SparseCategoricalEntireData&quot;
</span><span>_________________________________________________________________
</span><span> Layer (type)                Output Shape              Param #   
</span><span>=================================================================
</span><span> embedding_2 (Embedding)     (None, 50, 50)            486750    
</span><span>                                                                 
</span><span> flatten_2 (Flatten)         (None, 2500)              0         
</span><span>                                                                 
</span><span> dense_6 (Dense)             (None, 128)               320128    
</span><span>                                                                 
</span><span> dense_7 (Dense)             (None, 64)                8256      
</span><span>                                                                 
</span><span> dense_8 (Dense)             (None, 7)                 455       
</span><span>                                                                 
</span><span>=================================================================
</span><span>Total params: 815,589
</span><span>Trainable params: 328,839
</span><span>Non-trainable params: 486,750
</span><span>_________________________________________________________________
</span></code></pre>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span>model.weights[</span><span style="color:#d08770;">0</span><span>][</span><span style="color:#d08770;">1</span><span>][:</span><span style="color:#d08770;">10</span><span>], word_embeddings[</span><span style="color:#d08770;">1</span><span>][:</span><span style="color:#d08770;">10</span><span>]
</span></code></pre>
<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span>(&lt;tf.Tensor: shape=(10,), dtype=float32, numpy=
</span><span> array([ 0.15272 ,  0.36181 , -0.22168 ,  0.066051,  0.13029 ,  0.37075 ,
</span><span>        -0.75874 , -0.44722 ,  0.22563 ,  0.10208 ], dtype=float32)&gt;,
</span><span> array([ 0.15272   ,  0.36181   , -0.22168   ,  0.066051  ,  0.13029   ,
</span><span>         0.37075001, -0.75874001, -0.44722   ,  0.22563   ,  0.10208   ]))
</span></code></pre>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span>model.</span><span style="color:#bf616a;">fit</span><span>(X_train_vect, y_train, </span><span style="color:#bf616a;">batch_size</span><span>=</span><span style="color:#d08770;">32</span><span>, </span><span style="color:#bf616a;">epochs</span><span>=</span><span style="color:#d08770;">8</span><span>, </span><span style="color:#bf616a;">validation_data</span><span>=(X_test_vect, y_test))
</span></code></pre>
<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span>Epoch 1/8
</span><span>883/883 [==============================] - 6s 6ms/step - loss: 0.6581 - accuracy: 0.8125 - val_loss: 0.5072 - val_accuracy: 0.8456
</span><span>Epoch 2/8
</span><span>883/883 [==============================] - 5s 6ms/step - loss: 0.3699 - accuracy: 0.8839 - val_loss: 0.4601 - val_accuracy: 0.8626
</span><span>Epoch 3/8
</span><span>883/883 [==============================] - 5s 6ms/step - loss: 0.2727 - accuracy: 0.9141 - val_loss: 0.4704 - val_accuracy: 0.8636
</span><span>Epoch 4/8
</span><span>883/883 [==============================] - 5s 6ms/step - loss: 0.2132 - accuracy: 0.9316 - val_loss: 0.4715 - val_accuracy: 0.8747
</span><span>Epoch 5/8
</span><span>883/883 [==============================] - 5s 6ms/step - loss: 0.1715 - accuracy: 0.9457 - val_loss: 0.4943 - val_accuracy: 0.8779
</span><span>Epoch 6/8
</span><span>883/883 [==============================] - 5s 6ms/step - loss: 0.1461 - accuracy: 0.9529 - val_loss: 0.5230 - val_accuracy: 0.8780
</span><span>Epoch 7/8
</span><span>883/883 [==============================] - 6s 6ms/step - loss: 0.1356 - accuracy: 0.9551 - val_loss: 0.5588 - val_accuracy: 0.8762
</span><span>Epoch 8/8
</span><span>883/883 [==============================] - 5s 6ms/step - loss: 0.1132 - accuracy: 0.9634 - val_loss: 0.5902 - val_accuracy: 0.8782
</span><span>
</span><span>
</span><span>&lt;keras.callbacks.History at 0x7f5a97e5e0a0&gt;
</span></code></pre>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#b48ead;">from </span><span>sklearn.metrics </span><span style="color:#b48ead;">import </span><span>accuracy_score, classification_report, confusion_matrix
</span><span>
</span><span>y_preds = model.</span><span style="color:#bf616a;">predict</span><span>(X_test_vect).</span><span style="color:#bf616a;">argmax</span><span>(</span><span style="color:#bf616a;">axis</span><span>=-</span><span style="color:#d08770;">1</span><span>)
</span><span>
</span><span style="color:#96b5b4;">print</span><span>(&quot;</span><span style="color:#a3be8c;">Test Accuracy : </span><span style="color:#d08770;">{}</span><span>&quot;.</span><span style="color:#bf616a;">format</span><span>(</span><span style="color:#bf616a;">accuracy_score</span><span>(y_test, y_preds)))
</span><span style="color:#96b5b4;">print</span><span>(&quot;</span><span style="color:#96b5b4;">\n</span><span style="color:#a3be8c;">Classification Report : </span><span>&quot;)
</span><span style="color:#96b5b4;">print</span><span>(</span><span style="color:#bf616a;">classification_report</span><span>(y_test, y_preds, </span><span style="color:#bf616a;">target_names</span><span>=target_categories))
</span><span style="color:#96b5b4;">print</span><span>(&quot;</span><span style="color:#96b5b4;">\n</span><span style="color:#a3be8c;">Confusion Matrix : </span><span>&quot;)
</span><span style="color:#96b5b4;">print</span><span>(</span><span style="color:#bf616a;">confusion_matrix</span><span>(y_test, y_preds))
</span></code></pre>
<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span>295/295 [==============================] - 1s 2ms/step
</span><span>Test Accuracy : 0.8781731279872543
</span><span>
</span><span>Classification Report : 
</span><span>              precision    recall  f1-score   support
</span><span>
</span><span>       sport       0.83      0.86      0.85       992
</span><span>       autos       0.87      0.92      0.90      3428
</span><span>    religion       0.89      0.89      0.89      1312
</span><span>   comp_elec       0.91      0.89      0.90      1212
</span><span>     sci_med       0.90      0.84      0.87       988
</span><span>      seller       0.76      0.71      0.74       486
</span><span>    politics       0.93      0.83      0.88       997
</span><span>
</span><span>    accuracy                           0.88      9415
</span><span>   macro avg       0.87      0.85      0.86      9415
</span><span>weighted avg       0.88      0.88      0.88      9415
</span><span>
</span><span>
</span><span>Confusion Matrix : 
</span><span>[[ 857   70   14   11   15   12   13]
</span><span> [  70 3160   38   33   35   78   14]
</span><span> [  28   65 1165   36   10    3    5]
</span><span> [  13   61   41 1080    7    3    7]
</span><span> [  20   87   23   12  829    4   13]
</span><span> [  12  106    5    4    5  347    7]
</span><span> [  32   75   23   11   16   10  830]]
</span></code></pre>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#65737e;"># TensorFlow SavedModel format =&gt; .keras
</span><span>model_file = &#39;</span><span style="color:#a3be8c;">models/sparse_cat_entire</span><span>&#39;
</span><span>model.</span><span style="color:#bf616a;">save</span><span>(model_file)
</span></code></pre>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span>!pip install scikit-plot
</span><span style="color:#b48ead;">from </span><span>sklearn.metrics </span><span style="color:#b48ead;">import </span><span>confusion_matrix
</span><span style="color:#b48ead;">import </span><span>scikitplot </span><span style="color:#b48ead;">as </span><span>skplt
</span><span style="color:#b48ead;">import </span><span>matplotlib.pyplot </span><span style="color:#b48ead;">as </span><span>plt
</span><span style="color:#b48ead;">import </span><span>numpy </span><span style="color:#b48ead;">as </span><span>np
</span><span>
</span><span>skplt.metrics.</span><span style="color:#bf616a;">plot_confusion_matrix</span><span>([target_categories[i] </span><span style="color:#b48ead;">for </span><span>i </span><span style="color:#b48ead;">in </span><span>y_test], [target_categories[i] </span><span style="color:#b48ead;">for </span><span>i </span><span style="color:#b48ead;">in </span><span>y_preds],
</span><span>                                    </span><span style="color:#bf616a;">normalize</span><span>=</span><span style="color:#d08770;">True</span><span>,
</span><span>                                    </span><span style="color:#bf616a;">title</span><span>=&quot;</span><span style="color:#a3be8c;">Confusion Matrix</span><span>&quot;,
</span><span>                                    </span><span style="color:#bf616a;">cmap</span><span>=&quot;</span><span style="color:#a3be8c;">Greens</span><span>&quot;,
</span><span>                                    </span><span style="color:#bf616a;">hide_zeros</span><span>=</span><span style="color:#d08770;">True</span><span>,
</span><span>                                    </span><span style="color:#bf616a;">figsize</span><span>=(</span><span style="color:#d08770;">5</span><span>,</span><span style="color:#d08770;">5</span><span>)
</span><span>                                    );
</span><span>plt.</span><span style="color:#bf616a;">xticks</span><span>(</span><span style="color:#bf616a;">rotation</span><span>=</span><span style="color:#d08770;">90</span><span>);
</span></code></pre>
<p><img src="/images/glove/run_02.png#img-thumbnail" alt="png" /></p>
<h3 id="custom-test">Custom Test</h3>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#65737e;"># define documents
</span><span>docs = [&#39;</span><span style="color:#a3be8c;">Democrats the Reuplicans are both the worst!</span><span>&#39;,
</span><span>&#39;</span><span style="color:#a3be8c;">Coyotes win 10-0</span><span>&#39;,
</span><span>&#39;</span><span style="color:#a3be8c;">Houston Astros defeat the Cubs</span><span>&#39;,
</span><span>&#39;</span><span style="color:#a3be8c;">Apple and Microsoft both make great computers</span><span>&#39;,
</span><span>&#39;</span><span style="color:#a3be8c;">New washer 4sale. $200</span><span>&#39;]
</span><span>
</span><span>doc_array = np.</span><span style="color:#bf616a;">array</span><span>(docs)
</span><span>
</span><span>doc_array_vect  = </span><span style="color:#bf616a;">pad_sequences</span><span>(tokenizer.</span><span style="color:#bf616a;">texts_to_sequences</span><span>(doc_array), </span><span style="color:#bf616a;">maxlen</span><span>=max_tokens, </span><span style="color:#bf616a;">padding</span><span>=&quot;</span><span style="color:#a3be8c;">post</span><span>&quot;, </span><span style="color:#bf616a;">truncating</span><span>=&quot;</span><span style="color:#a3be8c;">post</span><span>&quot;, </span><span style="color:#bf616a;">value</span><span>=</span><span style="color:#d08770;">0.</span><span>)
</span><span>
</span><span>cstm_test_preds = model.</span><span style="color:#bf616a;">predict</span><span>(doc_array_vect).</span><span style="color:#bf616a;">argmax</span><span>(</span><span style="color:#bf616a;">axis</span><span>=-</span><span style="color:#d08770;">1</span><span>)
</span></code></pre>
<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span>1/1 [==============================] - 0s 18ms/step
</span></code></pre>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#96b5b4;">print</span><span>(doc_array)
</span></code></pre>
<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span>[&#39;Democrats the Reuplicans are both the worst!&#39; &#39;Coyotes win 10-0&#39;
</span><span> &#39;Houston Astros defeat the Cubs&#39;
</span><span> &#39;Apple and Microsoft both make great computers&#39; &#39;New washer 4sale. $200&#39;]
</span></code></pre>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#96b5b4;">print</span><span>(doc_array_vect)
</span></code></pre>
<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span>[[   1   48 2712    1  476    0    0    0    0    0    0    0    0    0
</span><span>     0    0    0    0    0    0    0    0    0    0    0    0    0    0
</span><span>     0    0    0    0    0    0    0    0    0    0    0    0    0    0
</span><span>     0    0    0    0    0    0    0    0]
</span><span> [  52  465   65    0    0    0    0    0    0    0    0    0    0    0
</span><span>     0    0    0    0    0    0    0    0    0    0    0    0    0    0
</span><span>     0    0    0    0    0    0    0    0    0    0    0    0    0    0
</span><span>     0    0    0    0    0    0    0    0]
</span><span> [7459 2362    1  988    0    0    0    0    0    0    0    0    0    0
</span><span>     0    0    0    0    0    0    0    0    0    0    0    0    0    0
</span><span>     0    0    0    0    0    0    0    0    0    0    0    0    0    0
</span><span>     0    0    0    0    0    0    0    0]
</span><span> [ 138    3  105 2712  279  402 1625    0    0    0    0    0    0    0
</span><span>     0    0    0    0    0    0    0    0    0    0    0    0    0    0
</span><span>     0    0    0    0    0    0    0    0    0    0    0    0    0    0
</span><span>     0    0    0    0    0    0    0    0]]
</span></code></pre>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#96b5b4;">print</span><span>(cstm_test_preds)
</span></code></pre>
<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span>[6 1 6 1 3]
</span></code></pre>


    </div>
  </section>
</body>

</html>